{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":507,"status":"ok","timestamp":1707050100501,"user":{"displayName":"Siddhi Patade","userId":"04618896576423067128"},"user_tz":-330},"id":"988fYqJ6A9wN"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from keras.models import Sequential\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, GRU, Dense, Dropout\n","from keras.optimizers import Adam\n","from keras.callbacks import EarlyStopping\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","# Load the dataset\n","url = \"https://raw.githubusercontent.com/mpasco/MalbehavD-V1/main/MalBehavD-V1-dataset.csv\"\n","data = pd.read_csv(url)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1400,"status":"ok","timestamp":1707050104233,"user":{"displayName":"Siddhi Patade","userId":"04618896576423067128"},"user_tz":-330},"id":"bLCn-piUA_Bd"},"outputs":[],"source":["# Assuming 'labels' is the column containing binary labels\n","y = data['labels']\n","\n","# Drop unnecessary columns and extract API call sequences\n","X_sequences = data.drop(['sha256', 'labels'], axis=1).apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n","\n","\n","# Tokenize the API call sequences\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X_sequences)\n","# Convert text to sequences\n","X_padded = pad_sequences(tokenizer.texts_to_sequences(X_sequences), padding='post')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":426,"status":"ok","timestamp":1707050106564,"user":{"displayName":"Siddhi Patade","userId":"04618896576423067128"},"user_tz":-330},"id":"qVnt828HBGGL"},"outputs":[],"source":["# Encode labels\n","label_encoder = LabelEncoder()\n","y_encoded = label_encoder.fit_transform(y)\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Data Augmentation (simplified)\n","datagen = ImageDataGenerator(\n","    rotation_range=10,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    vertical_flip=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from keras.models import load_model\n","from keras.callbacks import EarlyStopping\n","from keras.optimizers import Adam\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, GRU, Dense, Dropout\n","from keras.models import Sequential\n","import tensorflow as tf\n","\n","# Build the CNN-BiGRU hybrid model\n","embedding_dim = 100\n","filters = 256\n","kernel_size = 3\n","gru_units = 256\n","\n","model = Sequential()\n","\n","# Word Embedding Layer\n","model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=X_padded.shape[1]))\n","\n","# Convolutional Layer\n","model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","\n","# Bi-directional GRU Layer\n","model.add(Bidirectional(GRU(gru_units, dropout=0.5, recurrent_dropout=0.5)))\n","\n","# Additional Dense Layers\n","model.add(Dense(512, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(256, activation='relu'))\n","\n","# Output Layer\n","model.add(Dense(1, activation='sigmoid'))  # Binary classification\n","\n","# Compile the model with a smaller learning rate\n","model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Early stopping callback\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","# Train the model for more epochs\n","history = model.fit(X_train, y_train, epochs=11, batch_size=64, validation_data=(X_test, y_test))\n","\n","# Save the trained model with a specific file path and name\n","model_save_path = \"C:/Metamal-d-alert/MLmodel/model/malware_detection_model.h5\"\n","model.save(model_save_path)\n","print(f\"Model saved to {model_save_path}\")\n","\n","# loading the saved model for prediction\n","loaded_model = load_model(model_save_path)\n","\n","# Evaluate the loaded model\n","loss, accuracy = loaded_model.evaluate(X_test, y_test)\n","print(f\"Test Accuracy (Loaded Model): {accuracy * 100:.2f}%\")\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From C:\\Users\\Siddhi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n","WARNING:tensorflow:From C:\\Users\\Siddhi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n"]},{"ename":"NameError","evalue":"name 'tokenizer' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Word Embedding Layer\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Embedding(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mword_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39membedding_dim, input_length\u001b[38;5;241m=\u001b[39mX_padded\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Convolutional Layer\u001b[39;00m\n\u001b[0;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Conv1D(filters\u001b[38;5;241m=\u001b[39mfilters, kernel_size\u001b[38;5;241m=\u001b[39mkernel_size, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n","\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"]}],"source":["from keras.models import load_model, Sequential\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.optimizers import Adam\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, GRU, Dense, Dropout\n","import tensorflow as tf\n","\n","# Build the CNN-BiGRU hybrid model\n","\n","embedding_dim = 100\n","filters = 128  # Reducing the number of filters\n","kernel_size = 5  # Increasing kernel size for capturing broader context\n","gru_units = 128 \n","model = Sequential()\n","\n","# Word Embedding Layer\n","model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=X_padded.shape[1]))\n","\n","# Convolutional Layer\n","model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","\n","# Bi-directional GRU Layer\n","model.add(Bidirectional(GRU(gru_units, dropout=0.5, recurrent_dropout=0.5)))\n","\n","# Additional Dense Layers\n","model.add(Dense(512, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(256, activation='relu'))\n","\n","# Output Layer\n","model.add(Dense(1, activation='sigmoid'))  # Binary classification\n","\n","# Compile the model with a smaller learning rate\n","model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Early stopping callback\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","# Model checkpoint callback to save the best model during training\n","model_checkpoint = ModelCheckpoint(filepath=\"best_model.h5\", monitor='val_accuracy', save_best_only=True)\n","\n","# Train the model for more epochs with callbacks for early stopping and model checkpointing\n","history = model.fit(X_train, y_train, epochs=8, batch_size=32, validation_data=(X_test, y_test),\n","                    callbacks=[early_stopping, model_checkpoint])\n","\n","#  loading the saved model for prediction\n","loaded_model = load_model(\"best_model.h5\")\n","\n","# Evaluate the loaded model\n","loss, accuracy = loaded_model.evaluate(X_test, y_test)\n","print(f\"Test Accuracy (Loaded Model): {accuracy * 100:.2f}%\")\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPWFIyiGeq7OJzh/EeZX1Vi","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
